In this part, I tried to put together a few examples from my learning path towards 
  machine learning.

You have my attempt to implement and learn 'seq2seq', then explore the transformer 
  architecture, flash attention, and finally Nano-GPT.

Nano-GPT is the most meaningful implementation, heavily inspired by 
  Andrej Karpathy's work. In my version, I tried to give my own spin (to fit my 
  hardware/budget), but you can also see my notes and code changes as I attempted 
  to poke at everything to understand the intricacies of the model.

original repo: https://github.com/paulotcj/machine_learning/tree/main/models